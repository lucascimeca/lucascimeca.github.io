---
layout: default
---

# [](#Publications) Publications  <a href="https://orcid.org/0000-0002-2821-0072" class="ai ai-orcid" style="font-size: 25px; padding-left: 8px; color: #a3d13a" target="_blank"></a> <a href="https://scholar.google.co.uk/citations?user=fKJvAvMAAAAJ" class="ai ai-google-scholar" style="font-size: 25px; padding-left: 5px" target="_blank"></a>
<hr>

<br/>

## [](#bookchapters) Book Chapters
<hr style="width: 150px">

### Under Review (forthcoming)


> <div style="font-size: 13px"><b>Scimeca, L.</b>, & Iida, F. (forthcoming). Developmental soft robotics. MIT Press.</div>
<p></p>
> <div style="font-size: 13px"> Hughes, J., Birell, S., <b>Scimeca, L.</b>, & Iida, F. (forthcoming). Field Robotics for Harvesting. Elsevier.</div>
<p></p>


 <!-- ### Published-->
 
<br/>

## [](#journals) Journal Publications
<hr style="width: 208px">

### Under Review (forthcoming)

> <div style="font-size: 13px"><b>Scimeca, L.</b>, Hughes, J., Maiolino, P., He, L., Nanayakkara, T., & Iida, F. (forthcoming). Action Augmentation of Soft Tactile Perception for Soft-Body Palpation.</div>
<p></p>

> <div style="font-size: 13px"> He, L., Herzig, N., de Lusignan, S., <b>Scimeca, L.</b>, Maiolino, P., Iida, F., & T. Nanayakkara (forthcoming). An abdominal phantom with tunable stiffness nodules and force sensing capability for palpation training. <i>IEEE
Transactions on Robotics</i>. </div>
<p></p>

### Published 

<table> 
 
 <tr>
  <th style="padding: 5px 5px 20px 5px">
   2020
  </th>
  <th style="padding: 5px 5px 20px 5px">
   <blockquote>
    <div style="font-size: 13px; color: black; font-weight: normal" id='AURO2020'> 
     <b>Scimeca, L.</b>, Ng, C., & Iida, F. (2020). Gaussian Process inference modelling of dynamic robot
control for expressive piano playing. <i>PloS one</i>, 15(8), e0237826. Retrieved from <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0237826" target="_blank">link</a> 
    </div></blockquote>
  </th>
 </tr>
 
 <tr>
  <th style="padding: 5px 5px 20px 5px">
   2020
  </th>
  <th style="padding: 5px 5px 20px 5px">
   <blockquote>
    <div style="font-size: 13px; color: black; font-weight: normal" id='AURO2020'> 
     <b>Scimeca, L.</b>, Maiolino, P., Bray, E., & Iida, F. (2020). Structuring of tactile sensory information for category formation in robotics palpation. <i>Autonomous Robots</i>, 1-17. Retrieved from <a href="https://link.springer.com/article/10.1007/s10514-020-09931-y?fbclid=IwAR3-HFQuu6c-O-H51_W0PWGScdz_3JGICfCMGRWplCi66Zd5puwcDtCPRcQ" target="_blank">link</a> 
     
    </div>
   </blockquote>
  </th>
 </tr>

 <tr>
  <th style="padding: 5px 5px 20px 5px">
   2020
  </th>
  <th style="padding: 5px 5px 20px 5px">
   <blockquote>
    <div style="font-size: 13px; color: black; font-weight: normal" id='wrs2018'> 
     Hughes, J., Gilday, K., <b>Scimeca, L.</b>, Garg, S., & Iida, F. (2020). Flexible, adaptive industrial assembly: driving innovation through competition. <i>Intelligent Service Robotics</i>, 13(1), 169-178. Retrieved from <a href="https://link.springer.com/article/10.1007/s11370-019-00292-9" target="_blank">link</a> 
    </div>
   </blockquote>
  </th>
 </tr>
 
 <tr>
  <th style="padding: 5px 5px 20px 5px">
   2019
  </th>
  <th style="padding: 5px 5px 20px 5px">
   <blockquote>
    <div style="font-size: 13px; color: black; font-weight: normal">  
     <b>Scimeca, L.</b>, Hughes, J., Maiolino, P., & Iida, F. (2019). Model-free soft-structure reconstruction for proprioception using tactile arrays. <i>IEEE Robotics and Automation Letters</i>, 4(3), 2479-2484. Retrieved from <a href="https://ieeexplore.ieee.org/document/8675407" target="_blank">link</a> 
    </div>
   </blockquote>
  </th>
 </tr>
 
 <tr>
  <th style="padding: 5px 5px 20px 5px">
   2018
  </th>
  <th style="padding: 5px 5px 20px 5px">
   <blockquote>
    <div style="font-size: 13px; color: black; font-weight: normal"> 
     Hughes, J., <b>Scimeca, L.</b>, Ifrim, I., Maiolino, P., & Iida, F. (2018). Achieving robotically peeled lettuce. <i>IEEE Robotics and Automation Letters</i>, 3(4), 4337-4342. Retrieved from <a href="https://ieeexplore.ieee.org/abstract/document/8409969/" target="_blank">link</a> 
    </div>
   </blockquote>
  </th>
 </tr>
 
 <tr>
  <th style="padding: 5px 5px 20px 5px">
   2017
  </th>
  <th style="padding: 5px 5px 20px 5px">
   <blockquote>
    <div style="font-size: 13px; color:black; font-weight: normal"> 
     Stone, T., Webb, B., Adden, A., Weddig, N. B., Honkanen, A., Templin, R., Wcislo, W., <b>Scimeca, L.</b>, Warrant, E., & Heinze, S. (2017). An anatomically constrained model for path integration in the bee brain. <i>Current Biology</i>, 27(20), 3069-3085. Retrieved  from <a href="http://www.cell.com/current-biology/fulltext/S0960-9822(17)31090-4" target="_blank">link</a> 
    </div>
   </blockquote>
  </th>
 </tr>
</table>


<br/>


## [](#Conferences) Conference Proceedings
<hr style="width: 253px">

### Under Review (forthcoming)

> <div style="font-size: 13px"><b>L. Scimeca</b>, F. Iida, "Self-Supervised Learning Through Scene Observation for Selective Item Identification in Conveyor Belt Applications", in <i>Annual Conference Towards Autonomous Robotic Systems</i> (TAROS 2020) (forthcoming), Springer, Cham. </div>
<p></p>

### Published 

<table> 
 
 <tr>
  <th style="padding: 5px 5px 20px 5px">
   2020
  </th>
  <th style="padding: 5px 5px 20px 5px">
   <blockquote>
    <div style="font-size: 13px; color: black; font-weight: normal"> 
     <b>Scimeca, L.</b>, Iida, F., Maiolino, P., & Nanayakkara, T. (2020, March). Human-Robot Medical Interaction. In <i>Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction</i> (pp. 660-661). Retrieved from <a href="https://doi.org/10.1145/3371382.3374847" target="_blank">link</a> 
    </div>
   </blockquote>
  </th>
 </tr>
 
 <tr>
  <th style="padding: 5px 5px 20px 5px">
   2020
  </th>
  <th style="padding: 5px 5px 20px 5px">
   <blockquote>
    <div style="font-size: 13px; color: black; font-weight: normal"> 
     <b>Scimeca, L.</b>, Maiolino, P., & Iida, F. (2020, May). Efficient Bayesian Exploration for Soft Morphology-Action Co-optimization. In <i>2020 3rd IEEE International Conference on Soft Robotics (RoboSoft)</i> (pp. 639-644). IEEE. Retrieved from <a href="https://ieeexplore.ieee.org/abstract/document/9116057" target="_blank">link</a> 
    </div></blockquote>
  </th>
 </tr>
 
 <tr>
  <th style="padding: 5px 5px 20px 5px">
   2019
  </th>
  <th style="padding: 5px 5px 20px 5px">
   <blockquote>
    <div style="font-size: 13px; color: black; font-weight: normal"> 
     <b>Scimeca, L.</b>, Maiolino, P., Cardin-Catalan, D., del Pobil, A. P., Morales, A., & Iida, F. (2019, May). Non-destructive robotic assessment of mango ripeness via multi-point soft haptics. In <i>2019 International Conference on Robotics and Automation (ICRA)</i> (pp. 1821-1826). IEEE. Retrieved from <a href="https://ieeexplore.ieee.org/abstract/document/8793956" 
    </div>
   </blockquote>
  </th>
 </tr>
 
 <tr>
  <th style="padding: 5px 5px 20px 5px">
   2018
  </th>
  <th style="padding: 5px 5px 20px 5px">
   <blockquote>
    <div style="font-size: 13px; color: black; font-weight: normal" id='morph2018'> 
     <b>Scimeca, L.</b>, Maiolino, P., & Iida, F. (2018, April). Soft morphological processing of tactile stimuli for autonomous category formation. In <i> 2018 IEEE International  Conference on Soft Robotics (RoboSoft)</i> (pp. 356-361). IEEE. Retrieved  from <a href="https://ieeexplore.ieee.org/document/8404945/" target="_blank">link</a>
    </div> 
   </blockquote>
  </th>
 </tr>
 
 
</table>

<br/>

# [](#Projects) Other Projects <a href="https://bitbucket.org/lucascimeca/" class="fa fa-bitbucket" style="font-size: 25px; padding-left: 8px; color: #a3d13a" target="_blank"></a> <a href="https://github.com/lucascimeca/" class="fa fa-github" style="font-size: 25px; padding-left: 5px" target="_blank"></a>
<hr>

### [](#CodingWithNNs) Coding with Neural Networks<a href="https://bitbucket.org/lucascimeca/coding_with_neural_networks/src/master/" class="fa fa-external-link" style="font-size: 16px" target="_blank"></a><br/><font size="2px">(Massachusetts Institute of Technology, 2019)</font>

> <div style="font-style: italic; font-size: 12px">The project was developed as part of the "Center for Brains, Minds and Machines summer school 2019" and captures a novel compositional neural network achitecture devised during the course, and shown to be able to both learn to play "Game of Life" on arbitrarily long boards as well as perform "parity bit checking" on arbitrarily long bite strings. </div> <p></p>

### [](#WRS) Multiscale Object Recognition with Inception Neural Netoworks<a href="https://bitbucket.org/lucascimeca/multiscale_robotics_object_recognition/src/master/" class="fa fa-external-link" style="font-size: 16px" target="_blank"></a><br/><font size="2px">(University of Cambridge, 2018)</font>

> <div style="font-style: italic; font-size: 12px">Multiscale object recognition with custom inception neural networks in Tensor flow. The code was written for the Assembly challenge of the World Robotics Summit competition of 2018 in Tokyo, Japan. This work is linked to the <a href="https://lucascimeca.github.io/publications#wrs2018">Adaptive Industrial Assembly </a> publication in the journal Intelligent Service Robotics 2018. </div> 

### [](#MorphProc) Morphological Processing of Tactile Stimuli<a href="https://bitbucket.org/lucascimeca/morphological_processing/src/master/" class="fa fa-external-link" style="font-size: 16px" target="_blank"></a><br/><font size="2px">(University of Cambridge, 2017-2018)</font>

> <div style="font-style: italic; font-size: 12px">Project linked to the work on <a href="https://lucascimeca.github.io/publications#morph2018">Morphological Processing of Tactile Stimuli </a> published in the 2018 IEEE RoboSoft conference on soft robotics </div> 

### [](#RoboAnt) RoboAnt <a href="https://bitbucket.org/lucascimeca/antbot-a-biologically-inspired-approach-to-path-integration/raw/90d120d07c7fc6221d4ef454e8d6abab17988a72/Luca_Scimeca_AntBot.pdf" class="fa fa-download" style="font-size: 16px"></a><br/><font size="2px">(University of Edinburgh, 2016-2017)</font>

> <div style="font-style: italic; font-size: 12px">As part of my thesis at the university of Edinburgh I have worked with Dr. Barbara Webb on AntBot a mobile-phone powered autonomous robot implementing a novel neural model for insect navigation.</div> 

### [](#DNN) Image Classification With  DNN <a href="https://bitbucket.org/lucascimeca/deep-neural-networks-cifar-10-cifar-100-datasets?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3BGUJg49LJSNqF2TBvYYXHiQ%3D%3D" class="fa fa-external-link" style="font-size: 16px" target="_blank"></a><br/><font size="2px">(University of Edinburgh, 2016-2017)</font>

> <div style="font-style: italic; font-size: 12px">I created Tensor Flow Deep Neural Networks to classify colored images in the CIFAR-10 and CIFAR-100 datasets. I run experiments with particular focus on Multitask Learning and crated and adopted new learning rules for Multitask Learning on CNNs.</div>

### [](#DNN-mnist) Hand-written digit classification with DNN <a href="https://bitbucket.org/lucascimeca/deep-neural-networks-mnist-dataset?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base%3BGUJg49LJSNqF2TBvYYXHiQ%3D%3D" class="fa fa-external-link" style="font-size: 16px" target="_blank"></a><br/><font size="2px">(University of Edinburgh, 2016)</font>

> <div style="font-style: italic; font-size: 12px">I experimented with Adaptive Learning Rules, Batch Normalization, Convolutional Neural Networks (CNNs) and other standard NN approaches. I performed experiments on the MNIST dataset, achieving state of the art performance and increasing CNN forward and backward propagation speed of common algorithms of 2x (Python, no GPU).<div style="font-style: italic">

<br/>

 <div style="background-color:rgba(206, 206, 206, 1); color:black; font-weight:600">
 For a comprehensive list of projects please visit: <a href="https://bitbucket.org/lucascimeca/" target="_blank">https://bitbucket.org/lucascimeca/</a>
 </div>

<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>
<br/><br/>









